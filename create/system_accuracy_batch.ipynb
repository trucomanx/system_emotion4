{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f05b1d-f165-4197-a66e-030d883e169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # disable cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e77e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_default_json_conf_file='system_accuracy.json';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3410f40c-893b-449d-b024-7781e0d76733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c365c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9afbfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load json conf file\n",
    "fd = open(os.path.join('./',input_default_json_conf_file));\n",
    "DATA = json.load(fd);\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0f452-3278-49c0-9601-c8a443dbc32b",
   "metadata": {},
   "source": [
    "# Variable globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d83c3fa6-dbbb-4644-a12f-e497f971a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seed for the random variables\n",
    "seed_number=0;\n",
    "\n",
    "## Dataset \n",
    "dataset_base_dir     = DATA['dataset_base_dir'];\n",
    "dataset_labels_file  = DATA['dataset_labels_file'];\n",
    "dataset_name         = DATA['dataset_name'];\n",
    "\n",
    "checkpoint      = DATA['checkpoint'];\n",
    "model_type_body = DATA['model_type_body'];\n",
    "model_type_face = DATA['model_type_face'];\n",
    "model_type_skel = DATA['model_type_skel'];\n",
    "model_type_fusion = DATA['model_type_fusion'];\n",
    "enable_minus = DATA[\"enable_minus\"];\n",
    "\n",
    "# datafrem input dataset\n",
    "label_colname='label';\n",
    "filename_colname='filename';\n",
    "\n",
    "## Output\n",
    "output_base_dir = DATA['output_base_dir'];\n",
    "\n",
    "sub_dir='subdir';\n",
    "clean_break=200;\n",
    "\n",
    "body_factor=DATA['body_factor'];\n",
    "face_factor=DATA['face_factor'];\n",
    "face_detector_method=DATA['face_detector_method'];\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "##############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc12f1-6c56-4e35-b126-8979486b695b",
   "metadata": {},
   "source": [
    "# If command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2087e4ea-a8e4-4ed5-b2f7-2b391f054575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset_base_dir: /media/fernando/Expansion/DATASET/TESE/BER/BER2024-SOURCE\n",
      "dataset_labels_file: train.csv\n",
      "       dataset_name: ber2024\n",
      "    model_type_body: efficientnet_b3\n",
      "    model_type_face: efficientnet_b3\n",
      "    model_type_skel: 20\n",
      "  model_type_fusion: 11\n",
      "    output_base_dir: output\n",
      "            sub_dir: subdir\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(sys.argv)):\n",
    "    if   sys.argv[n]=='--dataset-dir':\n",
    "        dataset_base_dir=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--dataset-file':\n",
    "        dataset_labels_file=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--dataset-name':\n",
    "        dataset_name=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-check':\n",
    "        checkpoint=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-body':\n",
    "        model_type_body=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-face':\n",
    "        model_type_face=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-skel':\n",
    "        model_type_skel=int(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--model-type-fusion':\n",
    "        model_type_fusion=int(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--output-dir':\n",
    "        output_base_dir=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--sub-dir':\n",
    "        sub_dir=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--enable-minus':\n",
    "        enable_minus=sys.argv[n+1].lower()=='true';\n",
    "    elif sys.argv[n]=='--face-detector-method':\n",
    "        face_detector_method=int(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--face-factor':\n",
    "        face_factor=float(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--body-factor':\n",
    "        body_factor=float(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--clean-break':\n",
    "        clean_break=int(sys.argv[n+1]);\n",
    "\n",
    "INFO=dict();\n",
    "INFO[\"model_type_body\"] = model_type_body;\n",
    "INFO[\"model_type_face\"] = model_type_face;\n",
    "INFO[\"model_type_skel\"] = model_type_skel;\n",
    "INFO[\"model_type_fusion\"] = model_type_fusion;\n",
    "INFO[\"enable_minus\"] = enable_minus;\n",
    "INFO['body_factor'] = body_factor;\n",
    "INFO['face_factor'] = face_factor;\n",
    "INFO['face_detector_method'] = face_detector_method;\n",
    "\n",
    "# Serializar o dicionário em uma string JSON com uma ordenação consistente\n",
    "json_string = json.dumps(INFO, sort_keys=True)\n",
    "# Gerar um hash MD5 a partir da string JSON\n",
    "hash_object = hashlib.md5(json_string.encode())\n",
    "sub_dir = hash_object.hexdigest()\n",
    "\n",
    "print('   dataset_base_dir:',dataset_base_dir)\n",
    "print('dataset_labels_file:',dataset_labels_file)\n",
    "print('       dataset_name:',dataset_name)\n",
    "print('    model_type_body:',model_type_body)\n",
    "print('    model_type_face:',model_type_face)\n",
    "print('    model_type_skel:',model_type_skel)\n",
    "print('  model_type_fusion:',model_type_fusion)\n",
    "print('    output_base_dir:',output_base_dir)\n",
    "print('            sub_dir:',sub_dir)\n",
    "print('       enable_minus:',enable_minus)\n",
    "print('        clean_break:',clean_break)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1dee5-778a-4fd9-80de-90620bb33128",
   "metadata": {},
   "source": [
    "# Set seed of random variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ddef12f-6604-4c71-9473-15f328e954dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_number)\n",
    "tf.keras.utils.set_random_seed(seed_number);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5242bb-2077-4de0-8f41-374768f159e9",
   "metadata": {},
   "source": [
    "# Loading data of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f79564c-1ed0-4459-90cc-84e2bdda978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       filename     label\n",
      "0       person5/negative/frame_count23_cam1.png  negative\n",
      "1        person3/neutro/frame_count380_cam1.png    neutro\n",
      "2           person0/pain/frame_count68_cam1.png      pain\n",
      "3        person4/neutro/frame_count380_cam0.png    neutro\n",
      "4          person1/pain/frame_count131_cam0.png      pain\n",
      "...                                         ...       ...\n",
      "26954  person1/negative/frame_count262_cam2.png  negative\n",
      "26955     person7/neutro/frame_count20_cam2.png    neutro\n",
      "26956  person7/positive/frame_count219_cam1.png  positive\n",
      "26957   person5/negative/frame_count39_cam2.png  negative\n",
      "26958    person0/neutro/frame_count238_cam0.png    neutro\n",
      "\n",
      "[26959 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load filenames and labels\n",
    "df_full = pd.read_csv(os.path.join(dataset_base_dir,dataset_labels_file));\n",
    "print(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5189cf-b447-4b0a-b9f3-56f304d6fdde",
   "metadata": {},
   "source": [
    "# Creating output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a786de7-43ac-4597-92eb-d9eee66f81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = os.path.join(output_base_dir,\n",
    "                          dataset_name,\n",
    "                          sub_dir,\n",
    "                          'system_accuracy');\n",
    "\n",
    "os.makedirs(output_dir,exist_ok = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06008a2-e62c-44ed-9076-c4099b603978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o dicionário como JSON\n",
    "with open(os.path.join(output_dir,\"system_info.json\"), \"w\") as file:\n",
    "    json.dump(INFO, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb16f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../library');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018280cd",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "Info=dict();    \n",
    "Info[\"dataset_base_dir\"] = dataset_base_dir;\n",
    "Info[\"dataset_labels_file\"] = dataset_labels_file;\n",
    "Info[\"dataset_name\"] = dataset_name;\n",
    "Info[\"checkpoint\"] = checkpoint;\n",
    "Info[\"model_type_body\"] = model_type_body;\n",
    "Info[\"model_type_face\"] = model_type_face;\n",
    "Info[\"model_type_skel\"] = model_type_skel;\n",
    "Info[\"model_type_fusion\"] = model_type_fusion;\n",
    "Info[\"enable_minus\"]=enable_minus;\n",
    "Info['body_factor']=body_factor;\n",
    "Info['face_factor']=face_factor;\n",
    "Info['face_detector_method'] = face_detector_method;\n",
    "\n",
    "import platform\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    gpu_info = gpu_details.get('device_name')\n",
    "    Info[\"platform\"]=gpu_info;\n",
    "    print(f\"Modelo da GPU: {gpu_info}\")\n",
    "else:\n",
    "    cpu_info=platform.processor();\n",
    "    Info[\"platform\"]=cpu_info;\n",
    "    print(f\"Modelo da CPU: {cpu_info}\")\n",
    "\n",
    "print(json.dumps(Info, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf04e51-7a7b-4116-8e1e-04e5f358c5dc",
   "metadata": {},
   "source": [
    "# Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d629ba9-d621-4905-a88f-a0730d6c1802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layer with efficientnet_b3\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/FaceEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/FaceEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "Loading architecture efficientnet_b3\n",
      "\n",
      "        url: https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\n",
      "target_size: (300, 300)\n",
      "\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/BodyEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/BodyEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/SkeletonEmotion4Lib/models/model_onlycls_ncod20.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/SkeletonEmotion4Lib/models/model_onlycls_ncod20.h5\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/FusionEmotion4Lib/models/model_ncod11.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/FusionEmotion4Lib/models/model_ncod11.h5\n"
     ]
    }
   ],
   "source": [
    "import SystemEmotion4Lib.Classifier as sec\n",
    "\n",
    "cls=sec.Emotion4Classifier(checkpoint=checkpoint,\n",
    "                           model_type_face=model_type_face,\n",
    "                           model_type_body=model_type_body,\n",
    "                           model_type_skel=model_type_skel,\n",
    "                           model_type_skel_enable_minus=enable_minus,\n",
    "                           model_type_fusion=model_type_fusion,\n",
    "                           body_factor=body_factor,\n",
    "                           face_factor=face_factor,\n",
    "                           face_detector_method=face_detector_method);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67beeb38-1ad1-455d-8a71-5900a1e88163",
   "metadata": {},
   "source": [
    "# Creating PIL data images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2e3e68f-50e7-4f8c-9cd6-ae3261e8f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/26959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/openpifpaf/csrc/src/cif_hr.cpp:102: UserInfo: resizing cifhr buffer\n",
      "src/openpifpaf/csrc/src/occupancy.cpp:53: UserInfo: resizing occupancy buffer\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cfdfd3f2e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cfdfd3f2e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cfdfd3f3760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cfdfd3f3760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "acc:0.9705573080967402:   4%|▎         | 951/26959 [19:07<8:47:18,  1.22s/it] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "Count=0;\n",
    "N=0;\n",
    "L=len(df_full);\n",
    "\n",
    "output_filepath=os.path.join(output_dir,dataset_labels_file+\".json\");\n",
    "output_filepath_csv=os.path.join(output_dir,dataset_labels_file+\".output.csv\");\n",
    "\n",
    "initial_value=0;\n",
    "if os.path.exists(output_filepath_csv):\n",
    "    df = pd.read_csv(output_filepath_csv);\n",
    "    initial_value = len(df);\n",
    "\n",
    "#for index, row in tqdm(df_full.iterrows(), total=len(df_full), desc=\"Processing images\"):\n",
    "#for index, row in df_full.iterrows():\n",
    "#pbar=tqdm(df_full.iterrows(), total=len(df_full), desc=\"Processing images\");\n",
    "pbar=tqdm(initial=initial_value, total=L, desc=\"Processing images\");\n",
    "\n",
    "if initial_value==0:\n",
    "    with open(output_filepath_csv, mode='a', newline='') as arquivo_csv:\n",
    "        escritor = csv.writer(arquivo_csv)\n",
    "        escritor.writerow([filename_colname,label_colname,'predict','predict face','predict body','predict skeleton'])\n",
    "\n",
    "for Count in range(initial_value,L,batch_size):\n",
    "    # filepath\n",
    "    filenames = df_full.iloc[Count:(Count+batch_size)][filename_colname].tolist();\n",
    "    filepaths = [os.path.join(dataset_base_dir,fname) for fname in filenames];\n",
    "    pil_imgs  = [load_img(filepath) for filepath in filepaths];\n",
    "\n",
    "    # label_img\n",
    "    label_imgs = df_full.iloc[Count:(Count+batch_size)][label_colname].tolist();\n",
    "    label_imgs = [label.lower() for label in label_imgs];\n",
    "    \n",
    "    #pred, pred_face, pred_body, pred_skel = cls.from_img_all_pil(pil_img);\n",
    "    res, res_face, res_body, res_skel, face_bbox_list, body_bbox_list= cls.predict_all_pil_list(pil_imgs);\n",
    "    preds      = np.argmax(res     ,axis=1);\n",
    "    preds_face = np.argmax(res_face,axis=1);\n",
    "    preds_body = np.argmax(res_body,axis=1);\n",
    "    preds_skel = np.argmax(res_skel,axis=1);\n",
    "\n",
    "    reals = [cls.target_labels().index(label_img) for label_img in label_imgs];\n",
    "    \n",
    "    for pred,real,pred_face,pred_body,pred_skel,filename,label_img in zip(preds,reals,preds_face,preds_body,preds_skel,filenames,label_imgs):\n",
    "        if pred == real:\n",
    "            N += 1\n",
    "\n",
    "        with open(output_filepath_csv, mode='a', newline='') as arquivo_csv:\n",
    "            escritor = csv.writer(arquivo_csv)\n",
    "            lista = [   filename,\n",
    "                        label_img,\n",
    "                        cls.target_labels()[pred],\n",
    "                        cls.target_labels()[pred_face],\n",
    "                        cls.target_labels()[pred_body]];\n",
    "            if enable_minus:\n",
    "                lista.append('unknown');\n",
    "            else:\n",
    "                lista.append(cls.target_labels()[pred_skel]);\n",
    "            \n",
    "            escritor.writerow(lista);\n",
    "\n",
    "    if Count%clean_break==0:\n",
    "        torch.cuda.empty_cache();\n",
    "        tf.keras.backend.clear_session();\n",
    "    \n",
    "    pbar.update(len(filenames));\n",
    "    pbar.set_description(\"fast acc:%.5f\"%(N*1.0/(len(filenames)+Count-initial_value)))\n",
    "\n",
    "#ssInfo[\"iterations_per_second\"]=pbar.format_dict['rate'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_filepath_csv);\n",
    "\n",
    "# Comparar as colunas 'colA' e 'colB'\n",
    "iguais = df[label_colname] == df['predict'];\n",
    "Info[\"length\"]=L;\n",
    "Info[\"match\"]=int(iguais.sum());\n",
    "Info[\"accuracy\"] = float(iguais.mean());\n",
    "\n",
    "iguais = df[label_colname] == df['predict face'];\n",
    "Info[\"match_face\"]=int(iguais.sum());\n",
    "Info[\"accuracy_face\"] = float(iguais.mean());\n",
    "\n",
    "iguais = df[label_colname] == df['predict body'];\n",
    "Info[\"match_body\"]=int(iguais.sum());\n",
    "Info[\"accuracy_body\"] = float(iguais.mean());\n",
    "\n",
    "iguais = df[label_colname] == df['predict skeleton'];\n",
    "Info[\"match_skel\"]=int(iguais.sum());\n",
    "Info[\"accuracy_skel\"] = float(iguais.mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ACC:\",Info[\"accuracy\"]);\n",
    "\n",
    "print(json.dumps(Info, indent=4))\n",
    "\n",
    "with open(output_filepath, \"w\") as arquivo_json:\n",
    "    json.dump(Info, arquivo_json, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa44d5b5-d58a-46c7-a457-1d981e158e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "## Because the KERAS used is a old verion, KERAS 1\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "## Allows memory allocation on the GPU to be done asynchronously, \n",
    "## potentially improving performance and efficiency in certain scenarios.\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "## Specify which GPUs are available to a program.\n",
    "## The list of GPUs is usually represented by indices starting at 0.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # disable cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed521536-c3dd-4afd-a329-09522b2de9bf",
   "metadata": {},
   "source": [
    "# Install VideoImageTools\n",
    "\n",
    "To install VideoImageTools go to next link https://github.com/trucomanx/VideoImageTools\n",
    "\n",
    "# Install OpenPifPafTools\n",
    "\n",
    "To install OpenPifPafTools go to next link https://github.com/trucomanx/OpenPifPafTools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35643409",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_default_json_conf_file='testing_over_video.json';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec9de7-5010-475b-bed5-d18591b0964d",
   "metadata": {},
   "source": [
    "# Bibliotecas externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a25d22-37d7-493d-858b-402c00224ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import platform, sys, os\n",
    "\n",
    "import json\n",
    "import VideoImageTools as vit\n",
    "import OpenPifPafTools.OpenPifPafAnnotations as opp\n",
    "import OpenPifPafTools.OpenPifPafGetData as oppgd\n",
    "\n",
    "import cv2\n",
    "import openpifpaf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aea137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 22:12:51.761595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 22:12:52.301109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 22:12:52.713898: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-29 22:12:52.713914: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: fernando-B560M-DS3H-V2\n",
      "2024-08-29 22:12:52.713918: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: fernando-B560M-DS3H-V2\n",
      "2024-08-29 22:12:52.713946: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: 535.183.1\n",
      "2024-08-29 22:12:52.713959: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 535.183.1\n",
      "2024-08-29 22:12:52.713963: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:248] kernel version seems to match DSO: 535.183.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30331932-18cb-42c2-81f5-57edb1a4b5da",
   "metadata": {},
   "source": [
    "# Biblioteca local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b632af2f-ad01-445c-86ac-419456eba3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../library');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4259e7e-375f-4f8a-bb1f-2dabd5d876c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SystemEmotion4Lib.Classifier as mylib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f1671-d398-4639-a577-b835e5714131",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087eb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load json conf file\n",
    "fd = open(os.path.join('./',input_default_json_conf_file));\n",
    "DATA = json.load(fd);\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eefba85-28a6-450d-a3a7-c71d4b23dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model of network\n",
    "checkpoint = DATA[\"checkpoint\"];\n",
    "model_type_body = DATA[\"model_type_body\"]; ## 'mobilenet_v3', 'efficientnet_b3', 'inception_v3', 'inception_resnet_v2', 'resnet_v2_50'\n",
    "model_type_face = DATA[\"model_type_face\"]; ## 'mobilenet_v3', 'efficientnet_b3', 'inception_v3', 'inception_resnet_v2', 'resnet_v2_50'\n",
    "model_type_skel = DATA[\"model_type_skel\"];\n",
    "model_type_fusion = DATA[\"model_type_fusion\"];\n",
    "enable_minus = DATA[\"enable_minus\"];\n",
    "\n",
    "vin_path = DATA['input_mp4_file'];\n",
    "\n",
    "output_base_dir = DATA['output_base_dir'];\n",
    "\n",
    "sub_dir='sub_dir1';\n",
    "\n",
    "body_factor=1.0;\n",
    "face_factor=0.85;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfacf3b-046e-4426-b93f-fc4c335f1bf1",
   "metadata": {},
   "source": [
    "# If command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a8d6f8-1547-4b3c-bb99-9975c49dd53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       checkpoint: shufflenetv2k16\n",
      "  model_type_body: efficientnet_b3\n",
      "  model_type_face: efficientnet_b3\n",
      "  model_type_skel: 20\n",
      "model_type_fusion: 11\n",
      "     enable_minus: False\n",
      "         vin_path: /mnt/8811f502-ae19-4dd8-8371-f1915178f581/Fernando/DATASET/TESE/PATIENT-RECOGNITION/PATIENT-VIDEOS/dataset-toy/drhouse_mini_cut.mp4\n",
      "  output_base_dir: /mnt/8811f502-ae19-4dd8-8371-f1915178f581/Fernando/OUTPUTS/DOCTORADO2/system_emotion4_1\n",
      "          sub_dir: sub_dir1\n",
      "      body_factor: 1.0\n",
      "      face_factor: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(sys.argv)):\n",
    "    if   sys.argv[n]=='--model-type-check':\n",
    "        checkpoint=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-body':\n",
    "        model_type_body=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-face':\n",
    "        model_type_face=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--model-type-skel':\n",
    "        model_type_skel=int(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--model-type-fusion':\n",
    "        model_type_fusion=int(sys.argv[n+1]);\n",
    "    elif sys.argv[n]=='--enable-minus':\n",
    "        enable_minus=sys.argv[n+1].lower()=='true';\n",
    "    elif sys.argv[n]=='--input-file':\n",
    "        vin_path=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--output-dir':\n",
    "        output_base_dir=sys.argv[n+1];\n",
    "    elif sys.argv[n]=='--sub-dir':\n",
    "        sub_dir=sys.argv[n+1];\n",
    "\n",
    "\n",
    "print('')\n",
    "print('       checkpoint:',checkpoint);\n",
    "print('  model_type_body:',model_type_body);\n",
    "print('  model_type_face:',model_type_face);\n",
    "print('  model_type_skel:',model_type_skel);\n",
    "print('model_type_fusion:',model_type_fusion);\n",
    "print('     enable_minus:',enable_minus);\n",
    "print('         vin_path:',vin_path);\n",
    "print('  output_base_dir:',output_base_dir);\n",
    "print('          sub_dir:',sub_dir);\n",
    "print('      body_factor:',body_factor);\n",
    "print('      face_factor:',face_factor);\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440f607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vout_dir_path: /mnt/8811f502-ae19-4dd8-8371-f1915178f581/Fernando/OUTPUTS/DOCTORADO2/system_emotion4_1/sub_dir1/test_over_video\n"
     ]
    }
   ],
   "source": [
    "vout_dir_path=os.path.join(output_base_dir,sub_dir,'test_over_video');\n",
    "print('vout_dir_path:',vout_dir_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb19658-0095-4134-82bc-97b0ded311ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vout_path: /mnt/8811f502-ae19-4dd8-8371-f1915178f581/Fernando/OUTPUTS/DOCTORADO2/system_emotion4_1/sub_dir1/test_over_video/drhouse_mini_cut.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vout_path=os.path.join(vout_dir_path,os.path.basename(vin_path));\n",
    "print('vout_path:',vout_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adceab4-e767-415d-9a8b-9bd2373ba120",
   "metadata": {},
   "source": [
    "# Creating output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16c417d4-6024-40d1-b2bf-cc195c6695f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output_dir: /mnt/8811f502-ae19-4dd8-8371-f1915178f581/Fernando/OUTPUTS/DOCTORADO2/system_emotion4_1/sub_dir1/test_over_video\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(vout_dir_path,exist_ok=True); \n",
    "print('Created output_dir:',vout_dir_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d720a7-2636-4700-a973-d3487dea314c",
   "metadata": {},
   "source": [
    "# Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91fb32cf-a802-472d-a1d7-5f97c12ab6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "def extra_func(cap):\n",
    "    # Libera a memória da GPU no PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Limpa a sessão atual e libera a memória da GPU no TensorFlow\n",
    "    tf.keras.backend.clear_session()\n",
    "    print('clean')\n",
    "\n",
    "def my_func(Clf,frame):\n",
    "    #categories=['angry','disgusted','fearful','happy','neutral','sad','surprised'];\n",
    "    categories=['negative','neutral','pain','positive'];\n",
    "    \n",
    "    img_tmp = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB);\n",
    "    pil_im = Image.fromarray(img_tmp);\n",
    "    \n",
    "    res, res_face, res_body, res_skel, face_bbox, body_bbox = Clf.predict_all_pil(pil_im);\n",
    "\n",
    "    if res is None:\n",
    "        return frame;\n",
    "    \n",
    "    res = np.argmax(res);\n",
    "    texto=categories[res];\n",
    "    (xi,yi,xo,yo) = body_bbox;\n",
    "    color=(0,255,0);\n",
    "    thickness=2;\n",
    "    \n",
    "    frame = cv2.putText(frame,\n",
    "                        texto,\n",
    "                        org = (int(xi), int((yi+yo)/2)),\n",
    "                        fontFace = cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        fontScale = 2.0,\n",
    "                        color = (255, 0, 0),\n",
    "                        thickness = thickness\n",
    "                        );\n",
    "\n",
    "    cv2.rectangle(frame,(xi,yi),(xo,yo),color,thickness);\n",
    "    return frame;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4988749-3fbb-499e-8e56-e85486acdc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transfer learning architecture efficientnet_b3\n",
      "        url: https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\n",
      "target_size: (300, 300)\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/FaceEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/FaceEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "\n",
      "Transfer learning architecture efficientnet_b3\n",
      "        url: https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\n",
      "target_size: (300, 300)\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/BodyEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/BodyEmotion4Lib/models/model_efficientnet_b3.h5\n",
      "\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/SkeletonEmotion4Lib/models/model_onlycls_ncod20.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/SkeletonEmotion4Lib/models/model_onlycls_ncod20.h5\n",
      "\n",
      "Loading the weights in: /home/fernando/.local/lib/python3.10/site-packages/FusionEmotion4Lib/models/model_ncod11.h5\n",
      "Loaded the weights in: /home/fernando/.local/lib/python3.10/site-packages/FusionEmotion4Lib/models/model_ncod11.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  11%|████████████████▌                                                                                                                                        [ time left: 12:23 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  11%|████████████████▋                                                                                                                                        [ time left: 10:14 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  11%|████████████████▋                                                                                                                                        [ time left: 09:37 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  11%|█████████████████▍                                                                                                                                       [ time left: 14:00 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████                                                                                                                                   [ time left: 11:47 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▏                                                                                                                                  [ time left: 09:47 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▎                                                                                                                                  [ time left: 08:48 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▎                                                                                                                                  [ time left: 08:17 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▍                                                                                                                                  [ time left: 08:04 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▌                                                                                                                                  [ time left: 07:55 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▋                                                                                                                                  [ time left: 07:52 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▊                                                                                                                                  [ time left: 07:48 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▊                                                                                                                                  [ time left: 07:47 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|██████████████████████▉                                                                                                                                  [ time left: 07:47 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|███████████████████████                                                                                                                                  [ time left: 07:48 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|███████████████████████▏                                                                                                                                 [ time left: 07:48 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n",
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  15%|███████████████████████▏                                                                                                                                 [ time left: 07:48 ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person was found, will be returned All None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working:  16%|███████████████████████▊                                                                                                                                 [ time left: 15:43 ]"
     ]
    }
   ],
   "source": [
    "Clf=mylib.Emotion4Classifier(   checkpoint=checkpoint,\n",
    "                                model_type_face=model_type_face,\n",
    "                                model_type_body=model_type_body,\n",
    "                                model_type_skel=model_type_skel,\n",
    "                                model_type_skel_enable_minus=enable_minus,\n",
    "                                model_type_fusion=model_type_fusion,\n",
    "                                body_factor=body_factor,\n",
    "                                face_factor=face_factor);\n",
    "\n",
    "vit.apply_func_predictor_over_video_outmp4(my_func,Clf,vin_path,vout_path,extra_func_counter=1024,extra_func=extra_func)\n",
    "\n",
    "'''\n",
    "## Crio pasata com imagens\n",
    "vout_dir_tmp=os.path.join(vout_dir_path,os.path.splitext(os.path.basename(vin_path))[0]+'_'+model_type);\n",
    "image_files, fps = vit.apply_func_predictor_over_video_to_frames(my_func,(Clf,predictor),vin_path,vout_dir_tmp, show=False,FORMATO = \"frame_{:05d}.png\");\n",
    "\n",
    "## Save frames to video\n",
    "vit.images_to_video(image_files, fps, vout_path)\n",
    "'''\n",
    "\n",
    "print('working end')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
